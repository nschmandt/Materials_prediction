{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for vector position 1, the f_scores for cross-validation are as follows:\n",
      "[0.20000000000000001, 0.13333333333333333, 0.50000000000000011, 0.19999999999999998, 0.5714285714285714]\n",
      "True Positives:\n",
      "[1, 1, 4, 1, 4]\n",
      "False Positives\n",
      "[2, 0, 1, 0, 1]\n",
      "False Neg (missed positives)\n",
      "[6, 13, 7, 8, 5]\n",
      "for vector position 2, the f_scores for cross-validation are as follows:\n",
      "[0.25, 0.20000000000000001, 0.38461538461538458, 0.35294117647058826, 0.14814814814814817]\n",
      "True Positives:\n",
      "[2, 2, 5, 3, 2]\n",
      "False Positives\n",
      "[0, 1, 1, 0, 2]\n",
      "False Neg (missed positives)\n",
      "[12, 15, 15, 11, 21]\n",
      "for vector position 3, the f_scores for cross-validation are as follows:\n",
      "[0.71052631578947378, 0.676056338028169, 0.68103448275862066, 0.7207207207207208, 0.66326530612244905]\n",
      "True Positives:\n",
      "[81, 72, 79, 80, 65]\n",
      "False Positives\n",
      "[20, 24, 31, 21, 20]\n",
      "False Neg (missed positives)\n",
      "[46, 45, 43, 41, 46]\n",
      "for vector position 4, the f_scores for cross-validation are as follows:\n",
      "[0.40909090909090912, 0.44444444444444442, 0.48387096774193539, 0.26666666666666666, 0.36842105263157893]\n",
      "True Positives:\n",
      "[9, 12, 15, 6, 7]\n",
      "False Positives\n",
      "[3, 3, 3, 6, 3]\n",
      "False Neg (missed positives)\n",
      "[23, 27, 29, 27, 21]\n",
      "for vector position 5, the f_scores for cross-validation are as follows:\n",
      "[0.7246376811594204, 0.70646766169154218, 0.62672811059907829, 0.71028037383177567, 0.71356783919597999]\n",
      "True Positives:\n",
      "[75, 71, 68, 76, 71]\n",
      "False Positives\n",
      "[15, 16, 25, 17, 20]\n",
      "False Neg (missed positives)\n",
      "[42, 43, 56, 45, 37]\n",
      "for vector position 6, the f_scores for cross-validation are as follows:\n",
      "[0.22727272727272727, 0.49122807017543862, 0.48275862068965514, 0.40000000000000002, 0.48979591836734687]\n",
      "True Positives:\n",
      "[5, 14, 14, 11, 12]\n",
      "False Positives\n",
      "[6, 2, 7, 2, 5]\n",
      "False Neg (missed positives)\n",
      "[28, 27, 23, 31, 20]\n",
      "for vector position 7, the f_scores for cross-validation are as follows:\n",
      "[0.5636363636363636, 0.46296296296296297, 0.48275862068965519, 0.47169811320754718, 0.51327433628318586]\n",
      "True Positives:\n",
      "[31, 25, 28, 25, 29]\n",
      "False Positives\n",
      "[12, 10, 9, 9, 15]\n",
      "False Neg (missed positives)\n",
      "[36, 48, 51, 47, 40]\n",
      "for vector position 8, the f_scores for cross-validation are as follows:\n",
      "[0.62857142857142856, 0.5423728813559322, 0.56923076923076921, 0.61788617886178865, 0.51908396946564883]\n",
      "True Positives:\n",
      "[44, 32, 37, 38, 34]\n",
      "False Positives\n",
      "[12, 14, 14, 6, 19]\n",
      "False Neg (missed positives)\n",
      "[40, 40, 42, 41, 44]\n",
      "for vector position 9, the f_scores for cross-validation are as follows:\n",
      "[0.44444444444444442, 0.375, 0.47619047619047616, 0.375, 0.53333333333333333]\n",
      "True Positives:\n",
      "[4, 3, 5, 3, 4]\n",
      "False Positives\n",
      "[1, 1, 1, 1, 3]\n",
      "False Neg (missed positives)\n",
      "[9, 9, 10, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "#Let's start with a Random Forest Predictor, with 5 cross-validation sets\n",
    "#looping over all 9 of the classification predictions\n",
    "for j in range(9):\n",
    "    f_score_values=[]\n",
    "    true_pos_scores=[]\n",
    "    false_pos_scores=[]\n",
    "    false_neg_scores=[]\n",
    "    #Cross Validation Groups\n",
    "    for i in range(5):\n",
    "        temp=list(range(5))\n",
    "        temp.remove(i)\n",
    "        #this is our set that is removed from training for validation\n",
    "        val=training_nonoble[i::5]\n",
    "        #training set consists of the other values\n",
    "        train_X=[training_nonoble[temp[0]::5], training_nonoble[temp[1]::5], training_nonoble[temp[2]::5], training_nonoble[temp[3]::5]]\n",
    "        train_X=pd.concat(train_X)\n",
    "        model=RandomForestClassifier(500, n_jobs=-1)\n",
    "        fit=model.fit(train_X[col_names], train_X['stabilityVec%s' % j])\n",
    "        rf_prediction=fit.predict(val[col_names])\n",
    "        temp=rf_prediction-val['stabilityVec%s' % j]\n",
    "        false_pos=sum(temp==1)\n",
    "        false_neg=sum(temp==-1)\n",
    "        temp=rf_prediction+val['stabilityVec%s' % j]\n",
    "        true_pos=sum(temp==2)\n",
    "        precision=true_pos/(true_pos+false_pos)\n",
    "        recall=true_pos/(true_pos+false_neg)\n",
    "        f_score=2*precision*recall/(precision+recall)\n",
    "        f_score_values.append(f_score)\n",
    "        true_pos_scores.append(true_pos)\n",
    "        false_pos_scores.append(false_pos)\n",
    "        false_neg_scores.append(false_neg)\n",
    "    print('for vector position %s, the f_scores for cross-validation are as follows:' % str(j+1))\n",
    "    print(f_score_values)\n",
    "    print('True Positives:')\n",
    "    print(true_pos_scores)\n",
    "    print('False Positives')\n",
    "    print(false_pos_scores)\n",
    "    print('False Neg (missed positives)')\n",
    "    print(false_neg_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In all cases there are many more missed positives than false positives, suggesting the algorithm would do better \n",
    "#with a lower threshold for positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for vector position 1, the f_scores for cross-validation are as follows:\n",
      "[0.2857142857142857, 0.125, 0.36363636363636365, 0.40000000000000002, 0.375]\n",
      "True Positives:\n",
      "[2, 1, 4, 3, 3]\n",
      "False Positives\n",
      "[5, 1, 7, 3, 4]\n",
      "False Neg (missed positives)\n",
      "[5, 13, 7, 6, 6]\n",
      "for vector position 2, the f_scores for cross-validation are as follows:\n",
      "[0.31578947368421051, 0.33333333333333331, 0.46666666666666667, 0.40000000000000002, 0.3125]\n",
      "True Positives:\n",
      "[3, 4, 7, 4, 5]\n",
      "False Positives\n",
      "[2, 3, 3, 2, 4]\n",
      "False Neg (missed positives)\n",
      "[11, 13, 13, 10, 18]\n",
      "for vector position 3, the f_scores for cross-validation are as follows:\n",
      "[0.67256637168141598, 0.69565217391304346, 0.64516129032258063, 0.66666666666666674, 0.64893617021276606]\n",
      "True Positives:\n",
      "[76, 72, 70, 72, 61]\n",
      "False Positives\n",
      "[23, 18, 25, 23, 16]\n",
      "False Neg (missed positives)\n",
      "[51, 45, 52, 49, 50]\n",
      "for vector position 4, the f_scores for cross-validation are as follows:\n",
      "[0.35555555555555557, 0.3728813559322034, 0.33898305084745756, 0.34782608695652173, 0.2857142857142857]\n",
      "True Positives:\n",
      "[8, 11, 10, 8, 6]\n",
      "False Positives\n",
      "[5, 9, 5, 5, 8]\n",
      "False Neg (missed positives)\n",
      "[24, 28, 34, 25, 22]\n",
      "for vector position 5, the f_scores for cross-validation are as follows:\n",
      "[0.67632850241545894, 0.69306930693069302, 0.61818181818181817, 0.62376237623762376, 0.67692307692307696]\n",
      "True Positives:\n",
      "[70, 70, 68, 63, 66]\n",
      "False Positives\n",
      "[20, 18, 28, 18, 21]\n",
      "False Neg (missed positives)\n",
      "[47, 44, 56, 58, 42]\n",
      "for vector position 6, the f_scores for cross-validation are as follows:\n",
      "[0.2978723404255319, 0.4074074074074075, 0.41509433962264153, 0.40000000000000002, 0.36734693877551022]\n",
      "True Positives:\n",
      "[7, 11, 11, 11, 9]\n",
      "False Positives\n",
      "[7, 2, 5, 2, 8]\n",
      "False Neg (missed positives)\n",
      "[26, 30, 26, 31, 23]\n",
      "for vector position 7, the f_scores for cross-validation are as follows:\n",
      "[0.48543689320388356, 0.40776699029126207, 0.53448275862068972, 0.41176470588235292, 0.44444444444444442]\n",
      "True Positives:\n",
      "[25, 21, 31, 21, 24]\n",
      "False Positives\n",
      "[11, 9, 6, 9, 15]\n",
      "False Neg (missed positives)\n",
      "[42, 52, 48, 51, 45]\n",
      "for vector position 8, the f_scores for cross-validation are as follows:\n",
      "[0.54545454545454541, 0.46956521739130436, 0.56488549618320616, 0.58064516129032262, 0.46400000000000002]\n",
      "True Positives:\n",
      "[36, 27, 37, 36, 29]\n",
      "False Positives\n",
      "[12, 16, 15, 9, 18]\n",
      "False Neg (missed positives)\n",
      "[48, 45, 42, 43, 49]\n",
      "for vector position 9, the f_scores for cross-validation are as follows:\n",
      "[0.47619047619047622, 0.31578947368421051, 0.3571428571428571, 0.47619047619047622, 0.55555555555555558]\n",
      "True Positives:\n",
      "[5, 3, 5, 5, 5]\n",
      "False Positives\n",
      "[3, 4, 8, 4, 5]\n",
      "False Neg (missed positives)\n",
      "[8, 9, 10, 7, 3]\n"
     ]
    }
   ],
   "source": [
    "#Now let's try gradient boosting\n",
    "for j in range(9):\n",
    "    f_score_values=[]\n",
    "    true_pos_scores=[]\n",
    "    false_pos_scores=[]\n",
    "    false_neg_scores=[]\n",
    "    for i in range(5):\n",
    "        temp=list(range(5))\n",
    "        temp.remove(i)\n",
    "        val=training_nonoble[i::5]\n",
    "        train_X=[training_nonoble[temp[0]::5], training_nonoble[temp[1]::5], training_nonoble[temp[2]::5], training_nonoble[temp[3]::5]]\n",
    "        train_X=pd.concat(train_X)\n",
    "        model=GradientBoostingClassifier()\n",
    "        fit=model.fit(train_X[col_names], train_X['stabilityVec%s' % j])\n",
    "        gb_prediction=fit.predict(val[col_names])\n",
    "        temp=gb_prediction-val['stabilityVec%s' % j]\n",
    "        false_pos=sum(temp==1)\n",
    "        false_neg=sum(temp==-1)\n",
    "        temp=gb_prediction+val['stabilityVec%s' % j]\n",
    "        true_pos=sum(temp==2)\n",
    "        precision=true_pos/(true_pos+false_pos)\n",
    "        recall=true_pos/(true_pos+false_neg)\n",
    "        f_score=2*precision*recall/(precision+recall)\n",
    "        f_score_values.append(f_score)\n",
    "        true_pos_scores.append(true_pos)\n",
    "        false_pos_scores.append(false_pos)\n",
    "        false_neg_scores.append(false_neg)\n",
    "    print('for vector position %s, the f_scores for cross-validation are as follows:' % str(j+1))\n",
    "    print(f_score_values)\n",
    "    print('True Positives:')\n",
    "    print(true_pos_scores)\n",
    "    print('False Positives')\n",
    "    print(false_pos_scores)\n",
    "    print('False Neg (missed positives)')\n",
    "    print(false_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparing the two, the Gradient Boosting method seems to have better F-scores for more skewed datasets (ones with \n",
    "#fewer hits) while the RF method seemed to be slightly more accurate when predicting more balanced datasets.\n",
    "#Most likely, lowering the RF threshold would produce the best fitting model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
